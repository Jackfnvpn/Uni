### <span style="color:red">Teorema  
$\text{Siano } (x_0,y_0),(x_1,y_1),...,(x_n,y_n) \in \R^2 \ t.c \ x_0,x_1,...,x_n \text{ sono tutti distinti}.\newline \text{Allora } \exist! \ \text{polinomio } p(x) \in \R_n[x] \ t.c$ <span style="color:blue"> $\ p(x_i)=y_i \ \forall i=0,...,n$    

#### <span style="color:yellowgreen"> dim (2)</span>  

$\forall j=0,..,n$ definiamo il polinomio    

$L_j(x)=\prod^n_{i=0,i\neq j}\frac{x-x_i}{x_j-x_i}=\frac{(x-x_0)\cdots(x-x_{j-1})(x-x_{j+1})\cdots(x-x_n)}{(x_j-x_0)\cdots(x_j-x_{j-1})(x_j-x_{j+1})\cdots(x_j-x_n)}$  

I polinomi $L_0(x),L_1(x),\dots,L_n(x)$ sono in numero di $n+1$ e hanno tutti grado $n$, per cui sono $n+1$ elementi di $\R_n[x]$. Mostriamo che questi polinomi sono una base di $\R_n[x]$.   
>[!NOTE]  
>Ricordiamo che una base dello spazio vettoriale $\R_n[x]$ è un insieme di $V_1(x),...,V_n(x) \in \R_n[x]$ con le proprietà seguenti:  
> + sono linearmente indipendenti  
> + generano $\R_n[x]$, cioè ogni $q(x) \in \R_n[x]$ si può scrivere come combinazione lineare $\alpha_1v_1(x),\dots,\alpha_rv_r(x)$.  

Per far questo , mi basta dimostrare che essi sono linearmente indipendenti, perché essi sono nel numero giusto di $n+1=dim(\R_n[x])$.  


>[!NOTE]
> Tutte le basi di $\R_n[x]$ hanno lo stesso numero di elementi.  
> E questo numero di elementi comune a tutte le basi di $\R_n[x]$ si chiama dimensione di $\R_n[x]$. Poiché una base di $\R_n[x]$ famosa è la base canonica $1,x,x^2,\dots,x^n$ e ha $n+1$ elementi, deduciamo che la dimensione $dim(\R_n[x])=n+1$ 

>[!NOTE]
> Se si hanno $n+1$ elementi in uno spazio vettoriale di dim $n+1$, allora questi elementi sono una base dello spazio se e solo se sono lin. ind.  

Osserviamo che $L_0(x),L_1(x),\dots,L_n(x)$ hanno la seguente proprietà coincide: $\forall h,j=0,\dots,n$:  

$$L_j(x_h)=  
\begin{cases}
1 \ se \ h=j \\  
0 \ se \ h \neq j  \ \ \ \ \ (S)
\end{cases}
$$  
Dimostriamo  ora che $L_0(x),\dots,L_n(x)$ sono lin. ind.  
Sia $a_0L_0(x),a_1L_1(x)+\dots+a_nL_n(x)$ una combinazione lineare che coincide con il polinomio nullo, cioè t.c $a_0L_0(x),a_1L_1(x)+\dots+a_nL_n(x)=0 \ \forall x \in \R$. Allora $\forall i=0,\dots,n$ deve essere $a_0L_0(x),a_1L_1(x)+\dots+a_nL_n(x)=0$  

$=a_iL_i(x_i)=a_i \ \implies L_0(x),L_1(x),\dots,L_n(x)$ sono lin. ind e dunque una base di $\R_n[x]$  

Definiamo il polinomio $p(x)=y_0L_0(x)+y_1L_1(x)+\dots+y_nL_n(x)$.  
+ $p(x) \in \R_n[x]$
+ $\forall i=0,\dots,n, \ p(x_i)=y_0L_0(x_i)+y_1L_1(x_i)+\dots+y_nL_n(x_i)=y_i$  

Abbiamo dimostrato l'esistenza di un polinomio in $\R_n[x]$ che nei nodi $x_i$ assume i valori $y_i$  
Per dimostrare che $p(x)$ è l'unico polinomio in $\R_n[x]$ che soddisfa la condizione $p(x_i)=y_i \ \forall i=0,\dots,n$, supponiamo che $q(x)$ sia un altro polinomio in $\R_n[x]$ che soddisfa $q(x_i)=y_i \ \forall i=0,\dots,n$ e dimostriamo che $q(x)$ coincide con $p(x)$.  
Poiché $q(x) \in \R_n[x]$ e $L_0(x),\dots,L_n(x)$ sono una base di $\R_n[x]\ , \exist \beta_0,\dots,\beta_n \in \R \ t.c \ q(x)=\beta_0L_0(x)+\beta_1L_1(x)+\dots+\beta_nL_n(x)$.  
Poiché $q(x_i)=y_i \ \forall i =0,\dots,n$, deve essere che $\forall i=0,\dots,n$  

$y_i=q(x_i)=\beta_0L_0(x_i)+\beta_1L_1(x_i)+\dots+\beta_nL_n(x_i)=\beta_i \implies q(x)=y_0L_0+y_1L_1(x)+\dots+y_nL_n(x) \ \ \ \  _\square$.  


#### <span style="color:purple"> DEF </span>  

Siano $(x_0,y_0),(x_1,y_1),\dots,(x_n,y_n) \in \R^2$ con $x_0,x_1,\dots,x_n$ distinti.  
L'unico polinomio $p(x)\in \R_n[x] \ t.c \ p(x_i)=y_i \ \forall i=0,\dots,n$ si chiama polinomio d'interpolazione dei dati $(x_0,y_0),\dots,(x_n,y_n)$ o anche polinomio d'interpolazione dei valori $y_0,\dots,y_n$ sui nodi $x_0,\dots,x_n$.  
La prima dimostrazione del teorema precedente ci dice che $p(x)$ si scrive in ***FORMA CANONICA*** come  
$p(x)=a_0+a_1x+a_2x^2+\dots+a_nx^n$  con  

$$
\begin{bmatrix}
a_0\\  
a_1 \\
\vdots \\ 
a_n
\end{bmatrix}
\ =  
[V(x_0,x_1,\dots,x_n)]^{-1}  
\begin{bmatrix}
y_0\\  
y_1 \\
\vdots \\ 
y_n
\end{bmatrix}
$$  

e $[V(x_0,x_1,\dots \ x_n)]= \begin{bmatrix}
1 & x_0 &x{_0}^2 &\dots & x{_0}^n\\  
1 & x1 &x{_1}^2 & \dots & x{1}^n\\
\vdots & \vdots & \vdots & \vdots & \vdots \\ 
1 & x_n &x{_n}^2 & \dots & x{_n}^n\\
\end{bmatrix}$  

La seconda dimostrazione del teorema precedente ci dice che $p(x)$ si scrive in ***FORMA DI LAGRANGE*** come   
$p(x)=y_0L_0(x)+y_1L_1(x)+\dots+y_nL_n(x)$ dove $\forall j=0,\dots,n$ $(\$)$

$L_j(x)=j\text{-esimo}$ polinomio di lagrange relativo ai nodi $x_0,x_1,\dots,x_n=\prod^n_{i=0,i\neq j}\frac{x-x_i}{x_j-x_i}=\frac{(x-x_0)\dots(x-x_{j-1})(x-x_{j+1})\dots(x-x_n)}{(x_j-x_0)\dots(x_j-x_{j-1})(x_j-x_{j+1})\dots(x_j-x_n)}$.  

Se $y_0,\dots,y_n$ sono i valori nei punti $x_0,\dots,x_n$ di una funzione $f:[a,b]\rightarrow \R$, cioè se $y_i=f(x_i) \ \forall i=0,\dots,n$, allora l'unico polinomio $p(x)\in \R_n[x]\ t.c \ p(x_i)=f(x_i)\ \forall i=0,\dots,n$ si chiama anche polinomio d'interpolazione di $f(x)$ sui nodi $x_0,\dots,x_n$  

##### <span style="color:red"> Esempio</span>  
Scrivere in forma canonica e in forma di Lagrange il polinomio di interpolazione di $sin(x)$ sui nodi $x_0=0,x_1=\frac{\pi}{6}$, $x_2=\frac{\pi}{4}$  

##### <span style="color:blue"> Soluzione </span>   
Si inizia dalla forma di Lagrange $(\$)$, della quale si ha immediatamente che il polinomio d'interpolazione di $sin(x)$ su $x_0,x_1,x_2$ è

$p(x)=sin(x_0)L_0(x)+sin(x_1)L_1(x)sin(x_2)L_2=$  

$=sin(x_0)\frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}+sin(x_1)\frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)}+sin(x_2)\frac{(x-x_0)(x-x_1)}{(x_1-x_0)(x_2-x_1)}=$

$=\frac{1}{2} \frac{x(x-\frac{\pi}{4})}{\frac{\pi}{6}\cdot \frac{-\pi}{12}}+\frac{\sqrt{2}}{2}\frac{x(x-\frac{\pi}{6})}{\frac{\pi}{4}\frac{\pi}{12}}=$  

$= \frac{1}{2} \frac{x(x-\frac{\pi}{4})}{-\frac{\pi^2}{72}}+\frac{\sqrt{2}}{2}\frac{x(x-\frac{\pi}{6})}{\frac{\pi^2}{48}}$

Un controllo diretto permette di verificare che effettivamente: 
$$p(x_0)=sin(x_0)=0\\$$

$$p(x_1)=sin(x_1)=\frac{1}{2}\\$$

$$p(x_2)=sin(x_2)=\frac{\sqrt{2}}{2}$$  

Per scrivere $p(x)$ in forma canonica, sviluppiamo i calcoli a partire dalla forma di Lagrange:  
$p(x)=\frac{-24\sqrt{2}-36}{\pi^2}x^2+\frac{9-4\sqrt{2}}{\pi}x$  
>[!NOTE]  
Risolvendo $V(0,\frac{\pi}{6},\frac{\pi}{4})
\begin{bmatrix}
a_0 \\
a_1 \\
a_2 \\
\end{bmatrix}=
\begin{bmatrix}
0 \\
\frac{1}{2} \\
\frac{\sqrt{2}}{2} \\
\end{bmatrix}$  

, viene $\begin{bmatrix}
a_0 \\
a_1 \\
a_2 \\
\end{bmatrix}=\begin{bmatrix}
0 \\
\frac{-24\sqrt{2}-36}{\pi^2}\\
\frac{9-4\sqrt{2}}{\pi}x \\
\end{bmatrix}$  
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e669fbd",
   "metadata": {},
   "source": [
    "# Foresta Casuale\n",
    "Il modello utilizza un insieme di $k$ alberi decisionali addestrati separatamente. Per classificare, ognuno dei $k$ alberi assegna una etichetta; l'etichetta finale sarà quella più frequente. Se si fa in modo che tra gli alberi della foresta ci sia varietà, il modello risulterà essere molto stabile e robusto contro l'overfitting anche senza potatura degli alberi.\n",
    "\n",
    "Oltre agli Iperparametri utilizzati ereditati dagli alberi decisionali, avremo iperparametri specifici che sono: dimensione della foresta; dimensione degli insiemi di addestramento di ogni albero; numero di features utilizzate.\n",
    "\n",
    "## Addestramento e iperparametri\n",
    "\n",
    "Sia $C$ l'insieme di etichette che identificano le classi, $n$ il numero di esempi e $d$ il numero di caratteristiche. $X \\in R^{n \\times d}$ è la matrice che raccoglie il vettore delle caratteristiche per tutti gli esempi disponibili e $y \\in C^n$ il vettore delle classi a cui appartengano gli esempi.\n",
    "\n",
    "Sia $k$ il numero di alberi che compongono la foresta: per fare in modo che ci sia varietà tra gli alberi, ognuno di loro verrà addestrato su un sottoinsieme di dimensione $n'$ di esempi e un sottoinsieme di dimensione $d'$ di caratteristiche.\n",
    "\n",
    "Tipicamente l'insieme degli esempi utilizzato per addestrare un singolo albero decisionale viene scelto operando $n$ estrazioni casuali con reinserimento tra gli $n$ esempi del dataset originale: questa tecnica è detta di bootstrap. Invece, le $d'$ caratteristiche vengono scelte attraverso estrazioni senza reinserimento; un valore tipico per $d'$ è $\\sqrt{d}$.\n",
    "\n",
    "Le scelte descritte garantiscono una buona variabilità sugli alberi, per convincerci di questo concentriamoci soltanto su alcune proprietà del bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5195f26e",
   "metadata": {},
   "source": [
    "### Bootstrap\n",
    "Sia $I$ un sottoinsieme del dataset ottenuto con il bootstrap, calcoliamone la dimensione attesa per grandi valori di $n$.\n",
    "\n",
    "Introduciamo $n$ variabili aleatorie $x_i$ così definite\n",
    "\n",
    "$$ x_i = \\left\\{ \\begin{array}{lcl} 1 & & \\text{se l'esempio $i$ è in $I$}\\\\ 0 & & \\text{altrimenti} \\end{array} \\right. $$\n",
    "\n",
    "Dalla precedente,\n",
    "\n",
    "$$ |I| = \\sum_{i=1}^n x_i $$\n",
    "\n",
    "quindi\n",
    "\n",
    "$$ E(|I|) = \\sum_{i=1}^n E(x_i) $$\n",
    "\n",
    "Il valore medio di $x_i$ equivale alla probabilità che $x_i$ sia $1$ che equivale a\n",
    "\n",
    "$$ 1 - \\Pr(\\text{$i$ non venga mai scelto}) = 1 - \\left(1 - \\frac{1}{n}\\right)^n $$\n",
    "\n",
    "Al crescere di $n$, il secondo termine della differenza tende a $1/e$ da cui segue che\n",
    "\n",
    "$$ E(|I|) \\rightarrow n\\left( 1-\\frac{1}{e} \\right) \\approx 0.63 \\cdot n. $$\n",
    "\n",
    "Quindi circa il $37\\%$ degli esempi resta fuori (out of bag). Questi possono essere messi da parte ed utilizzati per testare il modello.\n",
    "\n",
    "Con il prossimo conto cerchiamo di capire quanto hanno in comune due sottoinsieme $I_0$ e $I_1$ ottenuti con la tecnica del bootstrap.\n",
    "\n",
    "Calcoliamo $E(|I_0\\cap I_1|)$. Analogamente a quanto fatto prima definiamo\n",
    "\n",
    "$$ x_i = \\left\\{ \\begin{array}{lcl} 1 & & \\text{se l'esempio $i$ è in $I_0\\cap I_1$}\\\\ 0 & & \\text{altrimenti} \\end{array} \\right. $$\n",
    "\n",
    "quindi $E(|I_0 \\cap I_1|) = \\sum_{i=1}^n E(x_i)$. Questa volta\n",
    "\n",
    "$$ Pr(x_i = 1) = 1 - Pr(x_i=0) $$\n",
    "\n",
    "$$ = Pr(\\text{$i$ non viene mai estratto per $I_0$ e non viene mai estratto per $I_1$}) $$\n",
    "\n",
    "$$ = \\left( 1-\\left( 1 - \\frac{1}{n}\\right)^n \\right)^2 $$\n",
    "\n",
    "$$ \\rightarrow \\left( 1-\\frac{1}{e} \\right)^2 \\approx 0.4 $$\n",
    "\n",
    "Quindi $E(|I_0 \\cap I_1|) \\approx 0.4\\cdot n$. In altre parole, ogni coppia di alberi viene addestrata su insiemi che differiscono tra loro per circa il $66\\%$ degli esempi. Questo, insieme alla scelta casuale delle feature contribuisce a creare alberi diversi tra di loro e questo favorisce il modello."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1652ee0",
   "metadata": {},
   "source": [
    "## Codice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26bf533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879fd444",
   "metadata": {},
   "source": [
    "### Classe Albero decisionale\n",
    "\n",
    "La modifica da apportare alla classe DecisionTree riguarda l'introduzione della possibilità di addestrare l'albero su un sottoinsieme delle caratteristiche piuttosto che su tutte. Tra i parametri è stato aggiunto num_feats che permette di speficicare quante features utilizzare. Queste verranno scelte a caso nel metodo __init__. La funzione utilizzata sarà choice di numpy.random con la quale verranno selezionati casualmente gli indici delle feature; il parametro replace = False indica che l'estrazione avviene senza reinserimento.\n",
    "\n",
    "L'altro metodo modificato è _get_best_split, utilizzato durante l'addestramento per aggiungere nel modo migliore i nodi all'albero. Nel ciclo principale, anziché iterare su tutti gli indici delle caratteristiche, itera su quelli selezionati in __init__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b4f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, max_depth=3, min_size=1, criterion='gini',\n",
    "                 num_feats = None, random_state = 0):      \n",
    "        impurity_funcs = {'gini': self._gini, 'entropy': self._entropy}\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.min_size = min_size\n",
    "        self.tree = None\n",
    "        \n",
    "        self._impurity_fun = impurity_funcs[criterion]\n",
    "            \n",
    "        self._num_feats = num_feats\n",
    "        self._random_state = random_state\n",
    "        \n",
    "        # indici delle features usate, in caso = None, tutte le features\n",
    "        self._feature_indxs = None\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Costruisce l'albero di decisione \"\"\"\n",
    "        y = np.array(y).reshape(-1, 1)  # anziché vettore, una matrice ad una colonna x tutte le righe che servono\n",
    "        \n",
    "        '''\n",
    "        dataset contiene sia X che y impilati verticalmente, questa è \n",
    "        la soluzione più conveniente per semplificare le operazioni\n",
    "        di filtro delle righe che porterà alle suddivisioni del dataset che\n",
    "        definiranno i nodi dell'albero\n",
    "        '''\n",
    "        \n",
    "        if self._num_feats!= None and self._num_feats < X.shape[1]:\n",
    "            rng = np.random.default_rng(seed=self._random_state)\n",
    "            # indici delle features usate\n",
    "            self._feature_indxs = rng.choice(np.arange(X.shape[1]), self._num_feats, replace=False)\n",
    "        else:\n",
    "            self._feature_indxs = np.arange(X.shape[1])\n",
    "            \n",
    "        \n",
    "        dataset =  np.hstack((X, y)) # Concatenazione orizzontale\n",
    "        self.tree = self._build_tree(dataset, 1)\n",
    "\n",
    "    def _info_gain(self, dataset, groups):\n",
    "        nl, nr = groups[0].shape[0], groups[1].shape[0]\n",
    "        n = nl + nr\n",
    "        ig = self._impurity_fun(dataset) - self._impurity_fun(groups[0])*nl/n - self._impurity_fun(groups[1])*nr/n\n",
    "        return ig\n",
    "        \n",
    "    def _entropy(self, dataset):\n",
    "        labs, occur = np.unique(dataset[:,-1], return_counts=True)\n",
    "        score = 0\n",
    "        for i, _ in enumerate(labs):\n",
    "            proportion = occur[i] / dataset.shape[0]\n",
    "            score += proportion * np.log2(proportion)\n",
    "        return -score\n",
    "    \n",
    "    def _gini(self, dataset):\n",
    "        labs, occur = np.unique(dataset[:,-1], return_counts=True)\n",
    "        score = 0\n",
    "        for i, _ in enumerate(labs):\n",
    "            proportion = occur[i] / dataset.shape[0]\n",
    "            score += proportion ** 2\n",
    "        return 1-score\n",
    "\n",
    "\n",
    "    def _split_dataset(self, index, value, dataset):\n",
    "        \"\"\" Divide il dataset in due gruppi in base al confronto della caratteristica\n",
    "        index con value\"\"\"\n",
    "        mask = dataset[:, index] < value\n",
    "        left, right = dataset[mask], dataset[~mask]\n",
    "\n",
    "        return left, right\n",
    "\n",
    "    def _get_best_split(self, dataset):\n",
    "        \"\"\" Trova la feature (colonna di dataset) sulla quale esiste un valore tale che  \n",
    "        Massimizza il guadagno informativo su tutte le possibili suddivisioni ottenibili usando\n",
    "        tutte le possibili caratteristiche.\n",
    "\n",
    "        Quindi per ogni caratteristica index e per ogni esempio row, si divide il dataset\n",
    "        in base al test x[index] < row[index] e se ne calcola  il guadagno informativo.\n",
    "        Si sceglie indx e row[index] in modo da massimizzare questo valore \n",
    "        \"\"\"\n",
    "        best_index, best_value, best_score, best_groups = None, None, float('-inf'), None\n",
    "        for index in self._feature_indxs: \n",
    "            for row in dataset:\n",
    "                groups = self._split_dataset(index, row[index], dataset)\n",
    "                ig = self._info_gain(dataset, groups)\n",
    "                if ig > best_score:\n",
    "                    best_index, best_value, best_score, best_groups = index, row[index], ig, groups\n",
    "\n",
    "        # ritorna un nudo\n",
    "        return {'index': best_index, 'value': best_value, 'groups': best_groups}\n",
    "\n",
    "    def _create_leaf(self, group):\n",
    "        \"\"\" Crea un nodo foglia con la classe più comune \"\"\"\n",
    "        values, counts = np.unique(group[:,-1], return_counts=True)\n",
    "        return values[np.argmax(counts)]\n",
    "\n",
    "    def _split(self, node, depth):\n",
    "        \"\"\" Cresce l'albero ricorsivamente \"\"\"\n",
    "        left, right = node['groups']\n",
    "        #del node['groups']\n",
    "        \n",
    "        # Se uno dei gruppi ï¿½ vuoto, assegniamo una foglia\n",
    "        if left.size == 0 or right.size == 0:\n",
    "            node['left'] = node['right'] = self._create_leaf(np.vstack( (left, right) ))\n",
    "            return\n",
    "\n",
    "        # Fermiamo la crescita se abbiamo raggiunto la profonditï¿½ massima\n",
    "        if depth >= self.max_depth:\n",
    "            node['left'], node['right'] = self._create_leaf(left), self._create_leaf(right)\n",
    "            return\n",
    "\n",
    "        # Se il gruppo sinistro ï¿½ troppo piccolo, creiamo una foglia\n",
    "        if len(left) <= self.min_size:\n",
    "            node['left'] = self._create_leaf(left)\n",
    "        else:\n",
    "            node['left'] = self._get_best_split(left)\n",
    "            self._split(node['left'], depth + 1)\n",
    "\n",
    "        # Se il gruppo destro ï¿½ troppo piccolo, creiamo una foglia\n",
    "        if len(right) <= self.min_size:\n",
    "            node['right'] = self._create_leaf(right)\n",
    "        else:\n",
    "            node['right'] = self._get_best_split(right)\n",
    "            self._split(node['right'], depth + 1)\n",
    "\n",
    "    def _build_tree(self, dataset, depth):\n",
    "        \"\"\" Costruisce l'albero a partire dai dati \"\"\"\n",
    "        root = self._get_best_split(dataset)\n",
    "        self._split(root, depth)\n",
    "        return root\n",
    "\n",
    "    def _is_leaf(self, node):\n",
    "        return not isinstance(node, dict)\n",
    "\n",
    "    def _predict_example(self, node, row):\n",
    "        \"\"\" Predice il valore di una singola riga \"\"\"\n",
    "        if row[node['index']] < node['value']:\n",
    "            if self._is_leaf(node['left']):\n",
    "                return node['left']\n",
    "            else:\n",
    "                return self._predict_example(node['left'], row)\n",
    "                \n",
    "        else:\n",
    "            if self._is_leaf(node['right']):\n",
    "                return node['right']\n",
    "            else:\n",
    "                return self._predict_example(node['right'], row)\n",
    "            \n",
    "    def predict(self, row):\n",
    "        \"\"\" Predice la classe di una singola riga \"\"\"\n",
    "        return self._predict_example(self.tree, row)\n",
    "\n",
    "    def predict_batch(self, X):\n",
    "        \"\"\" Predice su un intero dataset \"\"\"\n",
    "        return [self.predict(row) for row in X]\n",
    "    \n",
    "    def draw_tree(self):\n",
    "        self.the_tree = Digraph()\n",
    "    \n",
    "        def add_nodes_edges(node, parent_id=None, edge_lab = 'SI'):\n",
    "            if node is None:\n",
    "                return\n",
    "    \n",
    "            # Se foglia (intero)\n",
    "            if self._is_leaf(node):\n",
    "                node_id = str(id(node))\n",
    "                self.the_tree.node(node_id, str(node))\n",
    "                if parent_id:\n",
    "                    self.the_tree.edge(parent_id, node_id, edge_lab)\n",
    "                return\n",
    "    \n",
    "            # Nodo interno\n",
    "            node_id = str(id(node))\n",
    "            label = f\"f_{str(node.get('index',''))} < {str(node.get('value', ''))}\" \n",
    "            self.the_tree.node(node_id, label)\n",
    "    \n",
    "            if parent_id:\n",
    "                self.the_tree.edge(parent_id, node_id, edge_lab)\n",
    "    \n",
    "            add_nodes_edges(node.get('left'), node_id, 'SI')\n",
    "            add_nodes_edges(node.get('right'), node_id, 'NO')\n",
    "    \n",
    "        add_nodes_edges(self.tree)\n",
    "\n",
    "    def show_tree(self):\n",
    "        return self.the_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a622ee7",
   "metadata": {},
   "source": [
    "### Classe foresta casuale\n",
    "La seguente è una funzione di utilità, la moda, utilizzata per calcolare l'elemento più frequente tra quelli nella struttura in input. Oltre a ritornare l'elemento più frequente, ritorna anche il numero di volte in cui questo appare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa389f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode( a ):\n",
    "    '''\n",
    "    parametro: a, un array-like\n",
    "\n",
    "    return: l'elemento più frequente ed il numero di occorrenze\n",
    "\n",
    "    '''\n",
    "\n",
    "    itms, cnts = np.unique(np.array(a), return_counts=True )\n",
    "\n",
    "    return itms[np.argmax(cnts)], max(cnts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac551613",
   "metadata": {},
   "source": [
    "Il metodo __init__ della classe RandomForest inizializza gli iperparametri che sono:\n",
    "\n",
    "+ _n_trees: il numero di alberi della foresta (valore di default 3)\n",
    "+ _max_samples: il numero di esempi estratti dal dataset per formare l'insieme di addestramento di ogni singolo albero (valore di default il numero di righe del dataset)\n",
    "+ _max_feat_func: il numero di features utilizzate per addestrare gli alberi è ottenuto usando questa funzione applicata al numero di features totali (valore di default la funzione radice quadrata)\n",
    "Gli altri parametri servono per impostare gli iperparametri per il modello DecisionTree.\n",
    "\n",
    "La selezione degli esempi è eseguita nel metodi fit usando ancora una volta la funzione choice di numpy.random ma questa volta il parametro replace = True per eseguire estrazioni con reinserimento. Gli alberi vengono inseriti nella lista trees. Il metodo fit termina invocando il metodo _oob_valutation che esegue una valutazione del modello usando le istanze out-of-bag, ovvero quelle non estratte per l'addestramento. Più precisamente, durante l'addestramento, viene mantenuta aggiornata la struttura _used_rows che è un array di booleani di dimensione X.shape[0] ·_n_trees con la proprietà che _used_rows[i,t] vale True se e solo se l'esempio i è stato utilizzato per addestrare l'albero t. Il metodo _oob_valutation passa in rassegna tutti gli esempi e ognuno di questi viene valutato usando solo gli alberi per i quali non è stato utilizzato per l'addestramento. L'accuratezza ottenuta con questa procedura viene memorizzata nell'attributo _oob_accuracy.\n",
    "\n",
    "La classificazione di una istanza x avviene attraverso il metodo predict che raccoglie le le classificazioni da ogni albero nell'array predictions per poi restituire il valore più frequente in quest'ultimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4955c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    def __init__(self, n_trees = 3, max_samples = None,\n",
    "                 max_feat_func=np.sqrt, max_depth=3,\n",
    "                 min_size=1, criterion='gini',\n",
    "                 random_state = 0):\n",
    "        \n",
    "        self._n_trees = n_trees\n",
    "        self._max_samples = max_samples\n",
    "        self._max_feat_func = max_feat_func\n",
    "        \n",
    "        self._random_state = random_state\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.min_size = min_size        \n",
    "        self._impurity_fun = criterion\n",
    "\n",
    "        self.trees = []\n",
    "        self._used_rows = None # struttura usata per valutazione oob\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Numero di caratteristiche calcolate come funzione del\n",
    "        # numero di colonne di X\n",
    "        num_feats = int(self._max_feat_func(X.shape[1]))\n",
    "        rng = np.random.default_rng(seed=self._random_state)\n",
    "\n",
    "        self._used_rows = np.zeros( (X.shape[0], self._n_trees), dtype=bool)\n",
    "        # used_rows[i,t] vale True se la riga i è stata usata per\n",
    "        # addestrare l'albero t\n",
    "\n",
    "        if self._max_samples == None:\n",
    "            self._max_samples = X.shape[0]\n",
    "             \n",
    "        for i in range(self._n_trees):\n",
    "            rnd_rows = np.unique(rng.choice(np.arange(X.shape[0]), self._max_samples, replace=True))\n",
    "            self._used_rows[rnd_rows, i] = True # righe rnd_rows usate per albero i\n",
    "            tree = DecisionTree(max_depth = self.max_depth, \n",
    "                                min_size = self.min_size,\n",
    "                                criterion = self._impurity_fun,\n",
    "                                num_feats = num_feats,\n",
    "                                random_state = i\n",
    "                               )\n",
    "\n",
    "            Xf = X[rnd_rows, :]\n",
    "            yf = y[rnd_rows]\n",
    "    \n",
    "            tree.fit(Xf, yf)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "        self._oob_valutation(X, y)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        predictions = np.empty(self._n_trees, dtype=object)\n",
    "        \n",
    "        for i in range(self._n_trees):\n",
    "            predictions[i] = self.trees[i].predict(x)\n",
    "\n",
    "        return mode(predictions)\n",
    "        \n",
    "    def _oob_valutation(self, X, y):\n",
    "        '''\n",
    "        ogni esempio verrà utilizzato per valutare le prestazioni degli alberi di cui era out-of-bag\n",
    "        '''\n",
    "\n",
    "        successes, n = 0, 0 # numero di successi e numero di esperimenti (esempi oob almeno una volta)\n",
    "        \n",
    "        for i in range(X.shape[0]):        \n",
    "            xi, yi = X[i], y[i]\n",
    "\n",
    "            predictions_xi = []\n",
    "            \n",
    "            for j in range(self._n_trees): \n",
    "                if not self._used_rows[i][j]:  # xi non è stato usato per l'albero i\n",
    "                    predictions_xi.append(self.trees[j].predict(xi))\n",
    "\n",
    "            if predictions_xi != []:\n",
    "                outcome, _ = mode(predictions_xi)\n",
    "                successes += 1 if outcome == yi else 0\n",
    "                n += 1\n",
    "\n",
    "        self._oob_accuracy = successes/n\n",
    "\n",
    "    def get_accuracy(self):\n",
    "        return self._oob_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
